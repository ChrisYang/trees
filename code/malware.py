# Standard library
import unittest
from collections import Counter
from multiprocessing import Pool

# External libraries
import numpy as np
import matplotlib.pyplot as plt


class Record:
    'The aim of this class is to hide the representation of labels and'
    'records from the random forest building and classification algorithm'

    def __init__(self, labels, features, names=None):
        'Initialize with labels and records'
        self.labels = labels
        self.features = features
        if names is None:
            self.names = np.arange(len(labels), dtype=int)
        else:
            self.names = names

        assert len(self.labels) == self.features.shape[0]
        self.fI = self.get_indicator_function()

    def size(self):
        'The number of records'
        return self.features.shape[0]

    def indexes(self):
        return self.names

    def label_distribution(self):
        'The count of labels'
        return Counter(self.labels)

    def H(self):
        'Computes the binary entropy of labelled data'
        v = np.array(self.label_distribution().values())
        d = np.array(v) / float(sum(v))
        return - sum(d * np.log(d))

    def get_random_feature(self):
        'Select a random feature and a random threshold'
        rec_num, f_num = self.features.shape
        fID = np.random.random_integers(0, f_num-1)
        record = np.random.random_integers(0, rec_num-1)
        value = self.features[record, fID]
        return (fID, value)

    def get_indicator_function(self):
        'Returns the function used to decide left or right on any feature'
        def fI(records, F):
            fID, value = F
            return (records[:, fID] <= value)
        return fI

    def split_on_feature(self, feature):
        'Split the records according to a feature'

        # Define the indicator
        indicator = self.fI(self.features, feature)
        
        ## Split into two recrod sets
        labelsLeft = self.labels[indicator]
        featuresLeft = self.features[indicator, :]
        namesLeft = None
        if self.names is not None:
            namesLeft = self.names[indicator]

        labelsRight = self.labels[~indicator]
        featuresRight = self.features[~indicator, :]
        namesRight = None
        if self.names is not None:
            namesRight = self.names[~indicator]

        L = Record(labelsLeft, featuresLeft, namesLeft)
        R = Record(labelsRight, featuresRight, namesRight)

        ## What is the info gain?
        HL, SL = L.H(), L.size()
        HR, SR = R.H(), R.size()
        Ha, Sa = self.H(), float(self.size())

        dH = Ha - HL * (SL/Sa) - HR * (SR/Sa)
        return dH, L, R

## --------------------------
## - The random forest code -
## --------------------------


def build_tree(records, levels=5, numfeatures=100, cutoff=0.001):
    'Train a decision tree based on labeled data and features'
    if levels == 0 or records.H() == 0.0:
        C1 = records.label_distribution()
        Leaf = (None, C1)
        return Leaf
    else:
        gain = 0.0
        data = None
        for _ in xrange(numfeatures):
            F = records.get_random_feature()
            dH, L, R = records.split_on_feature(F)

            if gain < dH and cutoff < dH:
                gain = dH
                data = (F, L, R)

        if data is None:
            return build_tree(records, levels=0)

        (F, L, R) = data

        M1 = build_tree(L, levels - 1, numfeatures)
        M2 = build_tree(R, levels - 1, numfeatures)
        Branch = (F, M1, M2)
        return Branch


def parallel_build_tree(params):
    (labels, features, levels, numfeatures, cutoff, num_trees) = params
    R = Record(labels, features)
    root = []
    for t in xrange(num_trees):
        print "Make tree %s" % t
        T = build_tree(R, levels, numfeatures, cutoff)
        #print T
        root += [T]
    return root

class Forest:
    def __init__(self, trees=10, levels=5, numfeatures=100, cutoff=0.001):
        self.root = None

        self.trees = trees
        self.levels = levels
        self.numfeatures = numfeatures
        self.cutoff = cutoff

    def train(self, labels, features, workers=7):
        p = Pool(workers)
        per_worker = int(1 + (float(self.trees) / workers))
        params = [(labels, features, self.levels, self.numfeatures, self.cutoff, per_worker)] * workers
        x = p.map(parallel_build_tree, params)
        forest =  sum(x,[])[:self.trees]
        self.root = forest


    def classify(self, features):
        recs, _ = features.shape
        result_shape = (features.shape[0], len(self.root))
        scores = np.zeros(result_shape)
        print scores.shape
        R = Record(np.arange(recs, dtype=int), features)

        for i, T in enumerate(self.root):
            for idxs, result in classify(T, R):
                for idx in idxs.indexes():
                    scores[idx, i] = float(result[0]) / sum(result.values())


        plt.cla()
        plt.clf()
        plt.close()

        plt.imshow(scores, cmap=plt.cm.gray)
        plt.title('Scores matrix')
        plt.savefig(r"../scratch/tree_scores.png", bbox_inches='tight')

        return scores


def classify(tree, records):
    'Get a decision for an item using a tree'
    if len(tree) == 2:
        assert tree[0] is None
        yield (records, tree[1])
    else:
        F, LT, RT = tree
        _, L, R = records.split_on_feature(F)

        ## Check we did not drop any records
        assert records.size() == (L.size() + R.size())

        if L.size() > 0:
            for x in classify(LT, L):
                yield x
        if R.size() > 0:
            for x in classify(RT, R):
                yield x

def ROC_data(scores, labels, names, name="STD"):
    P = len(labels[labels==1])
    N = len(labels[labels==0])

    ## Make an ROC curve
    final_scores = np.mean(scores, axis=1)
    TP = [0.0]
    FP = [0.0]
    ACC = []
    for sc in sorted(set(final_scores)):
        L = labels[final_scores <= sc]
        Pi = len(L[L==1])
        Ni = len(L[L==0])
        tp = float(Pi) / P
        if tp == 0.0 or tp == 1.0:
            continue
        TP += [tp]
        FP += [float(Ni) / N]
        ACC += [(float(Pi) + (N - float(Ni)))/(P + N)]

    TP += [1.0]
    FP += [1.0]

    fp_regular = np.arange(0.0, 1.0, 0.001)
    tp_regular = np.zeros(len(fp_regular))
    j = 0
    for i, fp_i in enumerate(fp_regular):

        while not (FP[j] <= fp_i < FP[j+1]):
            j += 1

        #print i, fp_i, FP[j], TP[j], FP[j+1], TP[j+1]
        tp_i = (1 - ((FP[j+1] - fp_i) / (FP[j+1] - FP[j]))) * (TP[j+1] - TP[j]) + TP[j]
        tp_regular[i] = tp_i
        #print tp_i

    return max(ACC), tp_regular, fp_regular

def graph_ROC(max_ACC, TP, FP, name="STD"):
    aTP = np.vstack(TP)
    mean_TP = np.mean(aTP, axis=0)
    max_TP = np.amax(aTP, axis=0)
    min_TP = np.amin(aTP, axis=0)

    # sTP = sum(TP) / len(TP)
    sFP = FP[0]
    print len(sFP), len(mean_TP), len(TP[0])
    smax_ACC = np.mean(max_ACC)

    plt.cla()
    plt.clf()
    plt.close()

    plt.plot(sFP, mean_TP)
    plt.fill_between(sFP, min_TP, max_TP, color='black', alpha=0.2)
    plt.xlim((0,0.1))
    plt.ylim((0,1))
    plt.title('ROC Curve (accuracy=%.3f)' % smax_ACC)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.savefig(r"../scratch/"+name+"_ROC_curve.png", bbox_inches='tight')

    # Write the data to the file
    f = file(r"../scratch/"+name+"_ROC_curve.csv", "w")
    f.write("FalsePositive,TruePositive,Accuracy\n")
    for fp, tp in zip(sFP, mean_TP):
        f.write("%s,%s\n" % (fp, tp))
    f.close()


def ROC(scores, labels, names, name="STD"):

    max_ACC, TP, FP = ROC_data(scores, labels, names, name)
    graph_ROC([max_ACC], [TP], [FP], name)

    #P = len(labels[labels==1])
    #N = len(labels[labels==0])

    ## Save raw results in a file:
    #fr = file(r"../scratch/"+name+"_results.txt","w")
    #for s, l, n in sorted(zip(scores,labels, names), key=lambda x: np.mean(x[0])):
    #    fr.write("%.4f\t%s\t%s\n" % (np.mean(s), int(l), n))
    #fr.close()

    ## Make an ROC curve
    
    # acc_max = "%.2f" % max(ACC)

    #plt.cla()
    #plt.clf()
    #plt.close()

    #plt.plot(FP, TP)
    #plt.xlim((0,0.1))
    #plt.ylim((0,1))
    #plt.title('ROC Curve (accuracy=%.2f)' % max_ACC)
    #plt.xlabel('False Positive Rate')
    #plt.ylabel('True Positive Rate')
    #plt.savefig(r"../scratch/"+name+"_ROC_curve.png", bbox_inches='tight')

    #f = file(r"../scratch/"+name+"_ROC_curve.csv", "w")
    #f.write("FalsePositive,TruePositive,Accuracy\n")
    #for fp, tp, acc in zip(FP,TP, ACC):
    #    f.write("%s,%s,%s\n" % (fp, tp, acc))
    #f.close()



## Read the csv files
def read_data(labelsname, distancename):
    ## Extract labels
    rawlabels = np.genfromtxt(labelsname, delimiter=',', dtype=None)
    labelmap = {}
    row_len = 0
    for row in rawlabels:
        row_len = max(row_len, len(row)-1)
        name = row[0]
        labelmap[name] = list(row)[1:]

    ## Extract distances
    rawdistances = np.genfromtxt(distancename, delimiter=',', dtype=None)
    names = rawdistances[0][1:]
    distances = np.array(rawdistances[1:, 1:], dtype=float)
    labels = np.zeros((len(names), row_len))
    
    for i, name in enumerate(names):
        labels[i, 0:(len(row))] = labelmap[name]

    del labelmap
    return distances, labels, names


def visualizedistances(data, figname=None):
    D, L, N = data
    sorted_indexes = np.argsort(L[:,0])

    D2 = D[sorted_indexes, :]
    D2 = D2[:, sorted_indexes]

    plt.cla()
    plt.clf()
    plt.close()

    plt.imshow(D2, cmap=plt.cm.gray)
    plt.title('Distance matrix')
    plt.savefig(figname, bbox_inches='tight')


def selectsubsets(data, features=200, training=400, testing=100, fraction_negative = 0.5):
    D, L, N = data

    ## First identify the indexes of positives and negatives
    negatives = np.where(L[:,0] == 0)[0]
    positives = np.where(L[:,0] == 1)[0]

    Neg_training = 2 * int(training * fraction_negative) + features
    Pos_training = 2 * int(training * (1-fraction_negative)) + features

    np.random.shuffle(negatives)
    np.random.shuffle(positives)

    feature_set = np.hstack((negatives[:features], positives[:features]))
    training_set = np.hstack((negatives[features:Neg_training], positives[features:Neg_training]))

    test_size = testing # min(len(negatives[Neg_training:]), len(positives[Pos_training:]))
    
    test_set = np.hstack((negatives[Neg_training:Neg_training+test_size], positives[Pos_training:Pos_training+test_size]))

    assert len(test_set) == 2 * test_size
    print "Feature size: %s Training size: %s Test size: %s" % (len(feature_set), len(training_set), len(test_set))

    training_records = D[training_set, :]
    training_records = training_records[::, feature_set]
    training_records = np.hstack((training_records, L[training_set, 1:]))
    training_labels = L[training_set, 0]

    #training_labels = np.zeros(len(training_set), dtype=int)
    #training_labels[len(training_set)/2:] = np.ones(len(training_set)/2, dtype=int)

    print training_records.shape, training_labels.shape

    test_records = D[test_set, :]
    test_records = test_records[::, feature_set]
    test_records = np.hstack((test_records, L[test_set, 1:]))
    test_labels = L[test_set, 0]

    #test_labels = np.zeros(len(test_set), dtype=int)
    #test_labels[len(test_set)/2:] = np.ones(len(test_set)/2, dtype=int)

    training_data = (training_set, training_records, training_labels)
    test_data = (test_set, test_records, test_labels)

    return feature_set, training_data, test_data


class TestSeq(unittest.TestCase):

    def setUp(self):
        self.repeats = 30 # 10
        self.trees = 400 # 400
        self.features = 100
        self.training = 300
        self.testing = 300
        self.bias = 0.5

    def test_visualize(self):
        data = read_data('../data/filelabels.csv', '../data/ncdvals.csv')

        D, L, N = data
        assert D.shape == (2000, 2000)
        assert len(L) == 2000
        assert len(L[0]) == 1
        assert len(N) == 2000

        visualizedistances(data, '../scratch/distances.png')

    def classifier_test(self, data, D, L, N, name):
        repeats = self.repeats
        max_ACC, TP, FP = [], [], []
        for r in xrange(repeats):
            feature_set, training_data, test_data = selectsubsets(data, features=self.features, training=self.training, testing=self.testing, fraction_negative=self.bias)
            (training_set, training_records, training_labels) = training_data
            (test_set, test_records, test_labels) = test_data

            R = Record(training_labels, training_records, training_set)
            T = build_tree(R)

            for (r, d) in classify(T, R):
                assert r.label_distribution() == d

            F = Forest(trees = self.trees, numfeatures = 30)
            F.train(training_labels, training_records)
            scores = F.classify(test_records)

            max_ACCi, TPi, FPi = ROC_data(scores, test_labels, N[test_set], name)
            
            max_ACC += [ max_ACCi ]
            TP += [ TPi ] 
            FP += [ FPi ] 
            
        graph_ROC(max_ACC, TP, FP, name)
        return max_ACC, TP, FP


    def test_malwareanalysis(self):
        data = read_data('../data/filelabels.csv', '../data/ncdvals.csv')
        D, L, N = data
        self.classifier_test(data, D, L, N, name="MATRIX_ONLY")

    def test_malwareanalysis_unbalanced(self):
        data = read_data('../data/filelabels.csv', '../data/ncdvals.csv')
        D, L, N = data

        self.bias = 0.9
        self.classifier_test(data, D, L, N, name="MATRIX_ONLY_BIASED")

    def test_malwareanalysis_compress(self):
        data = read_data('../data/filelabels2.csv', '../data/ncdvals.csv')

        D, L, N = data
        assert len(L[0]) == 2
        
        self.classifier_test(data, D, L, N, name="MATRIX_COMPRESS")

    def test_malwareanalysis_compress_unbalanced(self):
        data = read_data('../data/filelabels2.csv', '../data/ncdvals.csv')

        D, L, N = data
        assert len(L[0]) == 2
        
        self.bias = 0.9
        self.classifier_test(data, D, L, N, name="MATRIX_COMPRESS_BIASED")


    def test_compress_only(self):
        data = read_data('../data/filelabels2.csv', '../data/ncdvals.csv')

        D, L, N = data
        
        self.features = 0
        self.classifier_test(data, D, L, N, name="COMPRESS_ONLY")


if __name__ == '__main__':
    unittest.main()